{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RELEBOHILE-PHEKO/autism-llm-assistant/blob/main/finetune_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dWO6SeFp8bPh",
      "metadata": {
        "id": "dWO6SeFp8bPh"
      },
      "source": [
        "#Domain-Specific Assistant: Early Autism Screening Guidance\n",
        " Fine-Tuning Gemma-2B-IT with QLoRA (4-bit + LoRA) on Google Colab\n",
        "\n",
        "Domain   : Healthcare — Early Childhood Autism Screening\n",
        " Model    : google/gemma-2b-it\n",
        " Method   : QLoRA (BitsAndBytes 4-bit + LoRA via PEFT)\n",
        " Framework: HuggingFace Transformers + PEFT + TRL + Gradio\n",
        "\n",
        " DISCLAIMER: For educational purposes only. Not a medical diagnosis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WhzRpDKC8aVq",
      "metadata": {
        "id": "WhzRpDKC8aVq"
      },
      "outputs": [],
      "source": [
        "#Install Dependencies\n",
        "\n",
        "get_ipython().system('pip install -q transformers datasets peft accelerate bitsandbytes trl evaluate sentencepiece nltk gradio pandas tabulate')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt',     quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "print(' Dependencies installed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ewIlsqHV9CsE",
      "metadata": {
        "id": "ewIlsqHV9CsE"
      },
      "source": [
        "# Imports & Global Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DNNRFOjj9Fxt",
      "metadata": {
        "id": "DNNRFOjj9Fxt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from math import ceil\n",
        "\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#  Global config\n",
        "MODEL_NAME   = 'google/gemma-2b-it'\n",
        "DATASET_PATH = 'data/autism_screening_guidance.jsonl'\n",
        "OUTPUT_DIR   = 'autism_guidance_gemma_2b'\n",
        "DEVICE       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f'Device : {DEVICE}')\n",
        "if DEVICE == 'cuda':\n",
        "    print(f'GPU    : {torch.cuda.get_device_name(0)}')\n",
        "    print(f'VRAM   : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # HuggingFace Login + Clone Repo & Generate Dataset"
      ],
      "metadata": {
        "id": "otlXlKi7PLsd"
      },
      "id": "otlXlKi7PLsd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fm_pM56X9XLo",
      "metadata": {
        "id": "fm_pM56X9XLo"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# Clone your repo and generate the dataset\n",
        "get_ipython().system('git clone https://github.com/RELEBOHILE-PHEKO/autism-llm-assistant')\n",
        "get_ipython().run_line_magic('cd', 'autism-llm-assistant')\n",
        "get_ipython().system('python create_dataset.py')\n",
        "\n",
        "# Verify file exists\n",
        "import os\n",
        "assert os.path.exists('data/autism_screening_guidance.jsonl'), \\\n",
        "    \" Dataset not found! Check create_dataset.py ran correctly.\"\n",
        "print(\" Dataset file confirmed.\")\n",
        "\n",
        "# Update path now we're inside the repo directory\n",
        "DATASET_PATH = 'data/autism_screening_guidance.jsonl'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fA4NXsR9haO",
      "metadata": {
        "id": "5fA4NXsR9haO"
      },
      "source": [
        "# Dataset Loading & Preprocessing\n",
        "\n",
        "## Preprocessing steps:\n",
        "  1. Load JSONL → HuggingFace Dataset\n",
        "  2. Drop incomplete rows (missing instruction or output)\n",
        "   3. Apply Gemma-2B-IT official chat template (<start_of_turn> tokens)\n",
        "  4. Analyse token-length distribution to justify MAX_SEQ_LENGTH\n",
        " 5. Filter sequences that exceed the context window\n",
        "  6. Create 90/10 train/eval split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dif8Pisb967u",
      "metadata": {
        "id": "Dif8Pisb967u"
      },
      "outputs": [],
      "source": [
        "def load_jsonl_dataset(path: str) -> Dataset:\n",
        "    \"\"\"Load a JSONL file into a HuggingFace Dataset.\"\"\"\n",
        "    data = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                data.append(json.loads(line))\n",
        "    print(f'Loaded {len(data):,} raw examples from {path}')\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "\n",
        "def format_example(example: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Apply Gemma-2B-IT official chat template.\n",
        "    <start_of_turn> / <end_of_turn> matches the model's pre-training format.\n",
        "    \"\"\"\n",
        "    instruction = (example.get('instruction') or '').strip()\n",
        "    output      = (example.get('output')      or '').strip()\n",
        "\n",
        "    if not instruction or not output:\n",
        "        return {'text': ''}\n",
        "\n",
        "    text = (\n",
        "        f'<start_of_turn>user\\n{instruction}<end_of_turn>\\n'\n",
        "        f'<start_of_turn>model\\n{output}<end_of_turn>'\n",
        "    )\n",
        "    return {'text': text}\n",
        "\n",
        "\n",
        "# Load & format\n",
        "raw_dataset  = load_jsonl_dataset(DATASET_PATH)\n",
        "print('Columns:', raw_dataset.column_names)\n",
        "print('Sample :', raw_dataset[0])\n",
        "\n",
        "formatted    = raw_dataset.map(format_example, remove_columns=raw_dataset.column_names)\n",
        "before_count = len(formatted)\n",
        "formatted    = formatted.filter(lambda x: x['text'].strip() != '')\n",
        "print(f'Kept {len(formatted):,} / {before_count:,} examples after empty-row filtering')\n",
        "\n",
        "#  Token-length analysis\n",
        "_tok             = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "_tok.pad_token   = _tok.eos_token\n",
        "\n",
        "lengths = [len(_tok(x['text'], truncation=False)['input_ids']) for x in formatted]\n",
        "p50     = sorted(lengths)[len(lengths) // 2]\n",
        "p90     = sorted(lengths)[int(len(lengths) * 0.90)]\n",
        "p95     = sorted(lengths)[int(len(lengths) * 0.95)]\n",
        "\n",
        "print(f'\\nToken length distribution:')\n",
        "print(f'  Min : {min(lengths)}')\n",
        "print(f'  p50 : {p50}')\n",
        "print(f'  p90 : {p90}')\n",
        "print(f'  p95 : {p95}')\n",
        "print(f'  Max : {max(lengths)}')\n",
        "\n",
        "# Set MAX_SEQ_LENGTH to cover p90+ of examples while keeping VRAM safe on T4\n",
        "MAX_SEQ_LENGTH = 256\n",
        "pct = sum(1 for l in lengths if l <= MAX_SEQ_LENGTH) / len(lengths) * 100\n",
        "print(f'\\nMAX_SEQ_LENGTH={MAX_SEQ_LENGTH} covers {pct:.1f}% of examples')\n",
        "\n",
        "formatted = formatted.filter(\n",
        "    lambda x: len(_tok(x['text'], truncation=False)['input_ids']) <= MAX_SEQ_LENGTH\n",
        ")\n",
        "print(f'Final dataset size: {len(formatted):,} examples')\n",
        "\n",
        "# Train / eval split\n",
        "split    = formatted.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = split['train']\n",
        "eval_ds  = split['test']\n",
        "\n",
        "print(f'\\nTrain : {len(train_ds):,} | Eval : {len(eval_ds):,}')\n",
        "print('\\nSample formatted text:')\n",
        "print(train_ds[0]['text'][:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KaZp4JVF-Av2",
      "metadata": {
        "id": "KaZp4JVF-Av2"
      },
      "source": [
        " # Load Base Model (4-bit QLoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7vITiTgW-KxY",
      "metadata": {
        "id": "7vITiTgW-KxY"
      },
      "outputs": [],
      "source": [
        "# NOTE: bf16=True in TrainingArguments requires bfloat16 compute dtype here\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,   # must match bf16=True in TrainingArguments\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer              = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token    = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "for cfg in [model.config, model.generation_config]:\n",
        "    cfg.eos_token_id = tokenizer.eos_token_id\n",
        "    cfg.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(' Gemma-2B-IT loaded (4-bit QLoRA ready).')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shared Generation Function"
      ],
      "metadata": {
        "id": "ROU06KrOQerO"
      },
      "id": "ROU06KrOQerO"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(mdl, tok, question: str, max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"Generate a response using the Gemma-2B-IT chat template.\"\"\"\n",
        "    prompt = (\n",
        "        f'<start_of_turn>user\\n{question}<end_of_turn>\\n'\n",
        "        f'<start_of_turn>model\\n'\n",
        "    )\n",
        "    inputs = tok(prompt, return_tensors='pt').to(mdl.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = mdl.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "    decoded    = tok.decode(outputs[0], skip_special_tokens=True)\n",
        "    # When skip_special_tokens=True, the output will be like \"user\\nQUESTION\\nmodel\\nANSWER\"\n",
        "    # We need to split by 'model\\n' to get only the answer part.\n",
        "    parts = decoded.split('model\\n')\n",
        "    if len(parts) > 1:\n",
        "        answer = parts[-1].strip()\n",
        "        return answer\n",
        "    return decoded.strip() # Fallback in case of unexpected format\n",
        "\n"
      ],
      "metadata": {
        "id": "wwF4ZXqwQbnH"
      },
      "id": "wwF4ZXqwQbnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MQ3C1rxr-SJs",
      "metadata": {
        "id": "MQ3C1rxr-SJs"
      },
      "source": [
        "#Baseline Evaluation (Pre Fine-Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M7BcHJ7c-Yca",
      "metadata": {
        "id": "M7BcHJ7c-Yca"
      },
      "outputs": [],
      "source": [
        "TEST_PROMPTS = [\n",
        "    'What are early signs of autism in a 2-year-old?',\n",
        "    'How is the M-CHAT-R screening tool used?',\n",
        "    'My child does not make eye contact at 18 months. Should I be concerned?',\n",
        "    'What developmental milestones should a toddler reach by age 2?',\n",
        "    'How can I support a child with autism at home?',\n",
        "]\n",
        "\n",
        "print('Generating BASE model outputs (before fine-tuning)...\\n')\n",
        "BASE_OUTPUTS = []\n",
        "for q in TEST_PROMPTS:\n",
        "    resp = generate_response(model, tokenizer, q)\n",
        "    BASE_OUTPUTS.append(resp)\n",
        "    print(f'Q: {q}')\n",
        "    print(f'A: {resp[:250]}\\n{\"─\"*60}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FITT-wNo-rgu",
      "metadata": {
        "id": "FITT-wNo-rgu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Training (Run 1 — Default Hyperparameters)\n",
        "\n",
        "# Rationale:\n",
        "  - lr=1e-5        : Conservative,\n",
        "\n",
        "avoids catastrophic forgetting\n",
        "\n",
        "-  batch=1+acc16  : Effective - - batch=16, safe for T4 VRAM\n",
        "-   epochs=2       : Enough convergence without overfitting\n",
        "-  bf16=True      : Matches bnb_4bit_compute_dtype=bfloat16\n",
        "-   cosine LR      : Smooth decay, better final loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6_uR7cA-6HL",
      "metadata": {
        "id": "_6_uR7cA-6HL"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE    = 1\n",
        "GRAD_ACC      = 16\n",
        "NUM_EPOCHS    = 2\n",
        "LORA_R        = 8\n",
        "LORA_ALPHA    = 16\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=['q_proj','k_proj','v_proj','o_proj',\n",
        "                    'gate_proj','up_proj','down_proj'],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM',\n",
        ")\n",
        "\n",
        "total_steps  = ceil(len(train_ds) / (BATCH_SIZE * GRAD_ACC)) * NUM_EPOCHS\n",
        "warmup_steps = int(0.05 * total_steps)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=False,\n",
        "    bf16=True,                          # consistent with bfloat16 compute dtype\n",
        "    logging_steps=20,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    report_to='none',\n",
        "    remove_unused_columns=False,\n",
        "    warmup_steps=warmup_steps,\n",
        "    lr_scheduler_type='cosine',\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    peft_config=lora_config            # SFTTrainer applies LoRA internally\n",
        ")\n",
        "\n",
        "t0           = time.time()\n",
        "train_result = trainer.train()\n",
        "train_time   = time.time() - t0\n",
        "\n",
        "print(f'\\n Training complete in {train_time / 60:.1f} minutes')\n",
        "print(f'Final train loss : {train_result.training_loss:.4f}')\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f'Model saved to: {OUTPUT_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m4GqQR-k_0Pn",
      "metadata": {
        "id": "m4GqQR-k_0Pn"
      },
      "source": [
        "#  Performance Metrics\n",
        "## Metrics used:\n",
        "  ### ROUGE-1  : Unigram overlap with reference answers\n",
        "  ### ROUGE-L  : Longest common subsequence overlap\n",
        "   ### BLEU     : Precision of n-gram matches (standard MT/NLG metric)\n",
        "   ### Perplexity: Model confidence on domain text (lower = better)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HfdCehF_Fz97",
      "metadata": {
        "id": "HfdCehF_Fz97"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WY5TfjBaAcB4",
      "metadata": {
        "id": "WY5TfjBaAcB4"
      },
      "outputs": [],
      "source": [
        "rouge_metric = load_metric('rouge')\n",
        "bleu_metric  = load_metric('bleu')\n",
        "\n",
        "\n",
        "def compute_metrics(predictions: list, references: list) -> dict:\n",
        "    \"\"\"Compute ROUGE-1, ROUGE-L, and BLEU.\"\"\"\n",
        "    rouge_scores = rouge_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=references,\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "    # bleu expects untokenized inputs for internal tokenization\n",
        "    bleu_score = bleu_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=[[r] for r in references],\n",
        "    )\n",
        "    return {\n",
        "        'rouge1': round(rouge_scores['rouge1'], 4),\n",
        "        'rougeL': round(rouge_scores['rougeL'], 4),\n",
        "        'bleu'  : round(bleu_score['bleu'],     4),\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_perplexity(mdl, tok, texts: list, max_len: int = 256) -> float:\n",
        "    \"\"\"Average perplexity over a list of text samples (lower = better).\"\"\"\n",
        "    mdl.eval()\n",
        "    total_loss = 0.0\n",
        "    for text in texts:\n",
        "        enc = tok(text, return_tensors='pt',\n",
        "                  truncation=True, max_length=max_len).to(mdl.device)\n",
        "        with torch.no_grad():\n",
        "            loss = mdl(**enc, labels=enc['input_ids']).loss\n",
        "        total_loss += loss.item()\n",
        "    return round(torch.exp(torch.tensor(total_loss / len(texts))).item(), 4)\n",
        "\n",
        "\n",
        "#  Build reference answers from raw eval split\n",
        "raw_split  = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "eval_raw   = raw_split['test'].select(range(min(50, len(raw_split['test']))))\n",
        "eval_refs  = [ex['output'] for ex in eval_raw]\n",
        "ref_sample = eval_refs[:5]             # aligned with the 5 TEST_PROMPTS\n",
        "\n",
        "# Fine-tuned outputs\n",
        "print('Generating fine-tuned outputs for metric evaluation...')\n",
        "FT_OUTPUTS = [generate_response(model, tokenizer, q) for q in TEST_PROMPTS]\n",
        "\n",
        "#  Scores\n",
        "base_scores = compute_metrics(BASE_OUTPUTS, ref_sample)\n",
        "ft_scores   = compute_metrics(FT_OUTPUTS,   ref_sample)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric'     : ['ROUGE-1', 'ROUGE-L', 'BLEU'],\n",
        "    'Base Model' : [base_scores['rouge1'], base_scores['rougeL'], base_scores['bleu']],\n",
        "    'Fine-Tuned' : [ft_scores['rouge1'],   ft_scores['rougeL'],   ft_scores['bleu']],\n",
        "})\n",
        "comparison_df['Δ Improvement'] = (\n",
        "    (comparison_df['Fine-Tuned'] - comparison_df['Base Model'])\n",
        "    / comparison_df['Base Model'].replace(0, 1e-9) * 100\n",
        ").round(1).astype(str) + '%'\n",
        "\n",
        "print('\\n=== Metric Comparison: Base vs Fine-Tuned ===')\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Perplexity\n",
        "eval_texts = [ex['text'] for ex in eval_ds.select(range(min(20, len(eval_ds))))]\n",
        "ft_ppl     = compute_perplexity(model, tokenizer, eval_texts)\n",
        "print(f'\\nFine-tuned perplexity (n=20): {ft_ppl}  (lower = better)')\n",
        "\n",
        "#  Qualitative side-by-side\n",
        "print('\\n' + '='*70)\n",
        "print('QUALITATIVE COMPARISON: Base vs Fine-Tuned')\n",
        "print('='*70)\n",
        "for i, q in enumerate(TEST_PROMPTS):\n",
        "    print(f'\\nQ{i+1}: {q}')\n",
        "    print(f'  BASE       : {BASE_OUTPUTS[i][:300]}')\n",
        "    print(f'  FINE-TUNED : {FT_OUTPUTS[i][:300]}')\n",
        "    print('─'*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ywbbCkAsop",
      "metadata": {
        "id": "65ywbbCkAsop"
      },
      "source": [
        "# Hyperparameter Experiments (3 Runs)\n",
        "\n",
        " Three runs varying: learning rate, LoRA rank, epochs.\n",
        "\n",
        "Each run reloads the base model from scratch for a fair comparison.\n",
        "\n",
        "Results are collected into an experiment table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c_evYFdA6vC",
      "metadata": {
        "id": "9c_evYFdA6vC"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def run_experiment(run_id, lr, lora_r, lora_alpha, epochs, grad_acc):\n",
        "    \"\"\"Train a fresh model with given hyperparameters and return metrics.\"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'RUN {run_id} | lr={lr} | lora_r={lora_r} | epochs={epochs} | grad_acc={grad_acc}')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    _model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME, quantization_config=bnb_config, device_map='auto'\n",
        "    )\n",
        "    _model.gradient_checkpointing_enable()\n",
        "    _model = prepare_model_for_kbit_training(_model)\n",
        "\n",
        "    _lora = LoraConfig(\n",
        "        r=lora_r, lora_alpha=lora_alpha,\n",
        "        target_modules=['q_proj','k_proj','v_proj','o_proj',\n",
        "                        'gate_proj','up_proj','down_proj'],\n",
        "        lora_dropout=0.05, bias='none', task_type='CAUSAL_LM',\n",
        "    )\n",
        "\n",
        "    _steps   = ceil(len(train_ds) / grad_acc) * epochs\n",
        "    _warmup  = int(0.05 * _steps)\n",
        "\n",
        "    _args = TrainingArguments(\n",
        "        output_dir=f'{OUTPUT_DIR}_run{run_id}',\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=grad_acc,\n",
        "        learning_rate=lr,\n",
        "        fp16=False, bf16=True,\n",
        "        logging_steps=50,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='no',\n",
        "        report_to='none',\n",
        "        remove_unused_columns=False,\n",
        "        warmup_steps=_warmup,\n",
        "        lr_scheduler_type='cosine',\n",
        "    )\n",
        "\n",
        "    _trainer = SFTTrainer(\n",
        "        model=_model, args=_args,\n",
        "        train_dataset=train_ds, eval_dataset=eval_ds,\n",
        "        peft_config=_lora,\n",
        "    )\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    t0      = time.time()\n",
        "    result  = _trainer.train()\n",
        "    elapsed = round((time.time() - t0) / 60, 1)\n",
        "\n",
        "    eval_out = _trainer.evaluate()\n",
        "    vram_gb  = torch.cuda.max_memory_allocated() / 1e9 if DEVICE == 'cuda' else 0\n",
        "\n",
        "    _preds  = [generate_response(_model, tokenizer, q) for q in TEST_PROMPTS]\n",
        "    _scores = compute_metrics(_preds, ref_sample)\n",
        "\n",
        "    del _model, _trainer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'Run'        : run_id,\n",
        "        'LR'         : lr,\n",
        "        'LoRA r'     : lora_r,\n",
        "        'LoRA alpha' : lora_alpha,\n",
        "        'Epochs'     : epochs,\n",
        "        'Grad Acc'   : grad_acc,\n",
        "        'Train Loss' : round(result.training_loss,   4),\n",
        "        'Eval Loss'  : round(eval_out['eval_loss'],  4),\n",
        "        'ROUGE-1'    : _scores['rouge1'],\n",
        "        'ROUGE-L'    : _scores['rougeL'],\n",
        "        'BLEU'       : _scores['bleu'],\n",
        "        'Time (min)' : elapsed,\n",
        "        'VRAM (GB)'  : round(vram_gb, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "exp_results = []\n",
        "\n",
        "# Run 1 — conservative LR, small rank\n",
        "exp_results.append(run_experiment(1, lr=1e-5,  lora_r=8,  lora_alpha=16, epochs=2, grad_acc=16))\n",
        "\n",
        "# Run 2 — higher LR, larger rank\n",
        "exp_results.append(run_experiment(2, lr=3e-5,  lora_r=16, lora_alpha=32, epochs=2, grad_acc=16))\n",
        "\n",
        "# Run 3 — aggressive LR, extra epoch\n",
        "exp_results.append(run_experiment(3, lr=1e-4,  lora_r=8,  lora_alpha=16, epochs=3, grad_acc=8))\n",
        "\n",
        "exp_df = pd.DataFrame(exp_results)\n",
        "print('\\n=== Hyperparameter Experiment Results ===')\n",
        "print(exp_df.to_string(index=False))\n",
        "\n",
        "best = exp_df.loc[exp_df['ROUGE-L'].idxmax()]\n",
        "print(f'\\n Best: Run {int(best[\"Run\"])} | LR={best[\"LR\"]} | '\n",
        "      f'LoRA r={int(best[\"LoRA r\"])} | ROUGE-L={best[\"ROUGE-L\"]} | BLEU={best[\"BLEU\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hBhXWGONBQoU",
      "metadata": {
        "id": "hBhXWGONBQoU"
      },
      "source": [
        "# Safety & Domain Guardrails\n",
        "\n",
        "## Two-layer protection:\n",
        "   1. Banned phrases  — blocks medical misinformation\n",
        "  2. Domain keywords — redirects off-topic queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XNEf5FUzBdM0",
      "metadata": {
        "id": "XNEf5FUzBdM0"
      },
      "outputs": [],
      "source": [
        "BANNED_PHRASES = [\n",
        "    'vaccines cause autism',\n",
        "    'cure autism',\n",
        "    'diagnose my child',\n",
        "    'autism is caused by bad parenting',\n",
        "]\n",
        "\n",
        "DOMAIN_KEYWORDS = [\n",
        "    'autism', 'asd', 'child', 'toddler', 'infant', 'baby',\n",
        "    'screening', 'development', 'speech', 'behavior', 'behaviour',\n",
        "    'milestone', 'social', 'm-chat', 'sensory', 'eye contact', 'nonverbal',\n",
        "]\n",
        "\n",
        "DISCLAIMER = (\n",
        "    '\\n\\n *General educational information only — '\n",
        "    'not a medical diagnosis. Please consult a licensed healthcare professional.*'\n",
        ")\n",
        "\n",
        "\n",
        "def safe_chat(question: str) -> str:\n",
        "    \"\"\"Apply guardrails then generate a response.\"\"\"\n",
        "    ql = question.lower()\n",
        "\n",
        "    if any(phrase in ql for phrase in BANNED_PHRASES):\n",
        "        return (\n",
        "            'I cannot provide medical diagnoses or spread misinformation. '\n",
        "            'Please consult a licensed healthcare professional or visit '\n",
        "            'cdc.gov/autism for trusted resources.' + DISCLAIMER\n",
        "        )\n",
        "\n",
        "    if not any(kw in ql for kw in DOMAIN_KEYWORDS):\n",
        "        return (\n",
        "            'I am designed to help with early autism screening and child '\n",
        "            'development guidance. Could you rephrase your question in '\n",
        "            'that context?' + DISCLAIMER\n",
        "        )\n",
        "\n",
        "    return generate_response(model, tokenizer, question) + DISCLAIMER\n",
        "\n",
        "\n",
        "# Quick tests\n",
        "print('Test 1 — Banned phrase:')\n",
        "print(safe_chat('vaccines cause autism')[:200])\n",
        "print('\\nTest 2 — Off-topic:')\n",
        "print(safe_chat('What is the capital of France?')[:200])\n",
        "print('\\nTest 3 — Valid query:')\n",
        "print(safe_chat('What are early signs of autism in a toddler?')[:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists('autism_guidance_gemma_2b'))\n",
        "print(os.listdir('autism_guidance_gemma_2b'))"
      ],
      "metadata": {
        "id": "Uj769Iu4X7lK"
      },
      "id": "Uj769Iu4X7lK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "t8OcHNQDBQkB",
      "metadata": {
        "id": "t8OcHNQDBQkB"
      },
      "source": [
        "#  Gradio User Interface\n",
        "Features:\n",
        "   - Conversation history (multi-turn)\n",
        "  - One-click example questions\n",
        "  - Send + Clear buttons\n",
        "   - Prominent medical disclaimer\n",
        "   - Links to authoritative resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CfrGuQH-Bma3",
      "metadata": {
        "id": "CfrGuQH-Bma3"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "EXAMPLE_QUESTIONS = [\n",
        "    'What are early signs of autism in a 2-year-old?',\n",
        "    'How is the M-CHAT-R screening tool used?',\n",
        "    'My child does not respond to their name at 12 months.',\n",
        "    'What developmental milestones should a toddler have by age 2?',\n",
        "    'How can I support my child with autism at home?',\n",
        "]\n",
        "\n",
        "\n",
        "def respond(message: str, history: list) -> tuple:\n",
        "    if not message.strip():\n",
        "        return '', history\n",
        "    reply = safe_chat(message)\n",
        "    history.append((message, reply))\n",
        "    return '', history\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    title='Early Autism Screening Guidance',\n",
        "    theme=gr.themes.Soft(primary_hue='blue'),\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        '#  Early Autism Screening Guidance Chatbot\\n'\n",
        "        '**Powered by Gemma-2B-IT fine-tuned with QLoRA**\\n\\n'\n",
        "        '>  **Medical Disclaimer:** General educational information only. '\n",
        "        'Not a substitute for professional medical advice or diagnosis. '\n",
        "        \"Always consult a licensed healthcare provider about your child's development.\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(label='Conversation', height=500)\n",
        "    msg_box = gr.Textbox(\n",
        "        placeholder='Ask about early autism signs, milestones, screening tools...',\n",
        "        label='Your question',\n",
        "        lines=2,\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button('Send ➤', variant='primary')\n",
        "        clear_btn  = gr.Button('Clear conversation')\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=EXAMPLE_QUESTIONS,\n",
        "        inputs=msg_box,\n",
        "        label='Example questions — click to use',\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\n",
        "        '---\\n'\n",
        "        '**Resources:** '\n",
        "        '[CDC Autism Info](https://www.cdc.gov/autism) · '\n",
        "        '[M-CHAT Screening](https://mchatscreen.com) · '\n",
        "        '[Autism Speaks](https://www.autismspeaks.org)'\n",
        "    )\n",
        "\n",
        "    submit_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "    msg_box.submit(respond,   [msg_box, chatbot], [msg_box, chatbot])\n",
        "    clear_btn.click(lambda: ([], ''), None, [chatbot, msg_box])\n",
        "\n",
        "demo.launch(share=True, debug=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "ozPzGCuGUDEB"
      },
      "id": "ozPzGCuGUDEB"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════╗\n",
        "║        Early Autism Screening Chatbot — Project Summary         ║\n",
        "╠══════════════════════════════════════════════════════════════════╣\n",
        "║ Model     : google/gemma-2b-it (QLoRA fine-tuned)               ║\n",
        "║ Domain    : Early childhood autism screening & guidance          ║\n",
        "║ Method    : 4-bit quantisation + LoRA via SFTTrainer            ║\n",
        "║ Metrics   : ROUGE-1, ROUGE-L, BLEU, Perplexity                 ║\n",
        "║ Guardrails: Banned phrases + domain keyword filter              ║\n",
        "║ UI        : Gradio Blocks — history, examples, disclaimer       ║\n",
        "╠══════════════════════════════════════════════════════════════════╣\n",
        "║ Key findings:                                                    ║\n",
        "║  • Fine-tuning improved ROUGE-L and BLEU over base model        ║\n",
        "║  • Conservative LR (1e-5 to 3e-5) outperformed 1e-4            ║\n",
        "║  • LoRA rank 16 / alpha 32 gave best response quality           ║\n",
        "║  • Guardrails block misinformation and off-topic queries        ║\n",
        "╚══════════════════════════════════════════════════════════════════╝\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "UUcJI0D5UBye"
      },
      "id": "UUcJI0D5UBye",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}