{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Early Autism Screening Guidance Chatbot\n",
        "## Fine-tuning TinyLlama 1.1B with LoRA\n",
        "\n",
        "**Project:** Academic - AI for Healthcare  \n",
        "**Target Users:** Caregivers, teachers, and community health workers  \n",
        "**Purpose:** Raise awareness, provide guidance, and encourage professional screening — **NOT for diagnosis**\n",
        "\n",
        "---\n",
        "### What this notebook does:\n",
        "1. Installs dependencies (transformers, peft, bitsandbytes, trl, etc.)\n",
        "2. Loads TinyLlama 1.1B with 4-bit quantization\n",
        "3. Loads and formats the instruction dataset (JSONL)\n",
        "4. Applies LoRA fine-tuning (memory-efficient)\n",
        "5. Trains with GPU-friendly settings\n",
        "6. Saves the fine-tuned model\n",
        "7. Provides inference + base vs fine-tuned comparison\n",
        "8. Evaluation: BLEU, ROUGE-L, perplexity, qualitative examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Run this cell first. On Colab: **Runtime → Change runtime type → GPU (T4 recommended)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (optimized for Colab/Kaggle free GPU)\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl scipy nltk evaluate sentencepiece\n",
        "\n",
        "# For evaluation metrics\n",
        "import nltk\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('punkt', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Config\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DATASET_PATH = \"data/autism_screening_guidance.jsonl\"  # Or upload your JSONL\n",
        "OUTPUT_DIR = \"autism_guidance_model\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load TinyLlama with 4-bit Quantization\n",
        "\n",
        "4-bit loading drastically reduces GPU RAM (from ~4GB to ~2GB base model load)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Prepare for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\"Model loaded with 4-bit quantization. Ready for LoRA.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Format Instruction Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72dc5337",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl_dataset(path: str) -> Dataset:\n",
        "    \"\"\"Load JSONL instruction dataset.\"\"\"\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line.strip())\n",
        "            data.append(item)\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "\n",
        "def format_prompt(example) -> str:\n",
        "    \"\"\"Format instruction + input + output for chat-style training.\"\"\"\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    inp = example.get(\"input\", \"\")\n",
        "    output = example.get(\"output\", \"\")\n",
        "    if inp:\n",
        "        text = f\"<|user|>\\n{instruction}\\n{inp}\\n<|assistant|>\\n{output}\"\n",
        "    else:\n",
        "        text = f\"<|user|>\\n{instruction}\\n<|assistant|>\\n{output}\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "\n",
        "# Load dataset. For Colab: upload data/ folder or run create_dataset.py first.\n",
        "if not Path(DATASET_PATH).exists():\n",
        "    Path(\"data\").mkdir(exist_ok=True)\n",
        "    try:\n",
        "        from create_dataset import create_dataset\n",
        "        create_dataset(DATASET_PATH)\n",
        "        print(\"Generated full dataset from create_dataset.py\")\n",
        "    except Exception:\n",
        "        D = \" This is not a diagnosis. Please consult a healthcare professional.\"\n",
        "        mini = [\n",
        "            {\"instruction\": \"What are early signs of autism?\", \"input\": \"\", \"output\": \"Early signs may include limited eye contact, delayed speech, repetitive behaviors, and reduced social interaction. Consider screening with a healthcare provider.\" + D},\n",
        "            {\"instruction\": \"When should I seek screening?\", \"input\": \"\", \"output\": \"Seek screening if you notice delayed speech, few gestures, or social differences. Routine screening at 18-24 months is recommended.\" + D},\n",
        "        ] * 50\n",
        "        with open(DATASET_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            for ex in mini: f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "        print(f\"Created minimal fallback dataset: {len(mini)} examples\")\n",
        "\n",
        "raw_dataset = load_jsonl_dataset(DATASET_PATH)\n",
        "formatted = raw_dataset.map(format_prompt, remove_columns=raw_dataset.column_names)\n",
        "\n",
        "print(f\"Loaded {len(formatted)} examples\")\n",
        "print(\"Sample formatted text:\")\n",
        "print(formatted[0][\"text\"][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Apply LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329a98c8",
      "metadata": {},
      "source": [
        "## 5a. Base Model Outputs (Run Before LoRA)\n",
        "\n",
        "Capture base model responses for later comparison. Run this before applying LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aad93ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, question, max_new_tokens=256, temperature=0.7, do_sample=True):\n",
        "    prompt = f\"<|user|>\\n{question}\\n<|assistant|>\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=do_sample, pad_token_id=tokenizer.eos_token_id)\n",
        "    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"<|assistant|>\" in full:\n",
        "        return full.split(\"<|assistant|>\")[-1].strip()\n",
        "    return full.strip()\n",
        "\n",
        "TEST_PROMPTS = [\"What are early signs of autism in toddlers?\", \"When should I seek autism screening?\", \"Is autism caused by vaccines?\"]\n",
        "BASE_OUTPUTS = [generate_response(model, tokenizer, q) for q in TEST_PROMPTS]\n",
        "print(\"Base outputs saved. Proceed to LoRA.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training\n",
        "\n",
        "Settings optimized for free Colab T4 (16GB) or Kaggle P100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Training complete. Model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    question: str,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ") -> str:\n",
        "    prompt = f\"<|user|>\\n{question}\\n<|assistant|>\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=do_sample,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"<|assistant|>\" in full:\n",
        "        return full.split(\"<|assistant|>\")[-1].strip()\n",
        "    return full.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Base vs Fine-tuned Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with BASE_OUTPUTS from Section 5a (run that before LoRA)\n",
        "try:\n",
        "    _ = TEST_PROMPTS\n",
        "except NameError:\n",
        "    TEST_PROMPTS = [\"What are early signs of autism in toddlers?\", \"When should I seek autism screening?\", \"Is autism caused by vaccines?\"]\n",
        "    BASE_OUTPUTS = [\"(Run Section 5a before LoRA to capture base outputs)\"] * 3\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASE vs FINE-TUNED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, q in enumerate(TEST_PROMPTS):\n",
        "    base_out = BASE_OUTPUTS[i] if i < len(BASE_OUTPUTS) else \"N/A\"\n",
        "    ft_out = generate_response(model, tokenizer, q)\n",
        "    print(f\"\\nQuestion: {q}\\n\")\n",
        "    print(f\"Base:      {(base_out[:400] + '...') if len(str(base_out)) > 400 else base_out}\")\n",
        "    print(f\"\\nFine-tuned: {(ft_out[:400] + '...') if len(ft_out) > 400 else ft_out}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluation Metrics\n",
        "\n",
        "Compute BLEU, ROUGE-L, and perplexity on a held-out subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(predictions, references):\n",
        "    bleu = corpus_bleu([[r.split()] for r in references], [p.split() for p in predictions])\n",
        "    rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "    return {\"bleu\": bleu, \"rouge_l\": rouge_result[\"rougeL\"]}\n",
        "\n",
        "# Sample eval set from dataset\n",
        "eval_data = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "eval_examples = eval_data[\"test\"][:50]\n",
        "\n",
        "references = [ex[\"output\"] for ex in eval_examples]\n",
        "predictions = [generate_response(model, tokenizer, ex[\"instruction\"]) for ex in eval_examples]\n",
        "\n",
        "metrics = compute_metrics(predictions, references)\n",
        "print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
        "print(f\"ROUGE-L: {metrics['rouge_l']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perplexity (on a small batch)\n",
        "def compute_perplexity(model, tokenizer, texts, max_length=256):\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    for t in texts[:20]:\n",
        "        inputs = tokenizer(t, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        total_loss += out.loss.item()\n",
        "        count += 1\n",
        "    avg_loss = total_loss / count if count else 0\n",
        "    return torch.exp(torch.tensor(avg_loss)).item()\n",
        "\n",
        "eval_texts = [ex[\"text\"] for ex in formatted.select(range(min(50, len(formatted))))]\n",
        "ppl = compute_perplexity(model, tokenizer, eval_texts)\n",
        "print(f\"Perplexity: {ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Experiments Table (Fill in after runs)\n",
        "\n",
        "| Experiment | LR | Batch | Grad Accum | Epochs | GPU RAM | Loss | BLEU | ROUGE-L | PPL |\n",
        "|------------|-----|-------|------------|--------|---------|------|------|---------|-----|\n",
        "| Base       | –   | –     | –          | –      | ~4GB    | –    | –    | –       | –   |\n",
        "| Exp 1      | 2e-5| 2     | 8          | 2      | ~8GB    | –    | –    | –       | –   |\n",
        "| Exp 2      | 5e-5| 4     | 4          | 1      | ~9GB    | –    | –    | –       | –   |\n",
        "\n",
        "Use this table in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Interactive Chat (Quick Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(question: str) -> str:\n",
        "    return generate_response(model, tokenizer, question)\n",
        "\n",
        "# Example:\n",
        "print(chat(\"What are early signs of autism in toddlers?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Gradio UI (uncomment to use)\n",
        "# import gradio as gr\n",
        "# gr.Interface(fn=chat, inputs=\"text\", outputs=\"text\", title=\"Autism Screening Guidance\").launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
